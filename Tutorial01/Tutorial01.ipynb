{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutorial for Machine Learning using Python (Pandas and Scikit-Learn)\n",
    "===================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [파이썬](https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%8D%AC) (파이선? 파이쏜?)\n",
    "\n",
    "[파이썬](https://www.python.org/) (Python)은 1991년 프로그래머인 [귀도 반 로섬(Guido van Rossum)](https://www.google.com/search?client=firefox-b-d&q=guido+van+rossum)이 발표한 [고급 프로그래밍 언어](https://ko.wikipedia.org/wiki/%EA%B3%A0%EA%B8%89_%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D_%EC%96%B8%EC%96%B4)로 플랫폼 독립적이며 인터프리터식, 객체지향적, [동적 타이핑(dynamically typed](https://stackoverflow.com/questions/1517582/what-is-the-difference-between-statically-typed-and-dynamically-typed-languages)) 대화형 언어이다. 파이썬이라는 이름은 귀도가 좋아하는 코미디 [<Monty Python's Flying Circus>](https://www.google.com/search?client=firefox-b-d&q=Monty+Python%27s+Flying+Circus)에서 따온 것이다.\n",
    "\n",
    "[Hey Guido~ !](https://www.google.com/search?q=guido+cars+movie&client=firefox-b-d&source=lnms&tbm=isch&sa=X&ved=0ahUKEwj24_OGv-_hAhUDUrwKHWnLCVMQ_AUIDigB&biw=1527&bih=1103)\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Import \n",
    "\n",
    "#### 파이선 사용을 위해서는 사용할 라이브러리를 불러오는 것을 먼저 실행해야 합니다. \n",
    "  * numpy \n",
    "    + object for matrix, verctor and its operations \n",
    "  * pandas\n",
    "    + data manipulation, especially, csv (excel ...)\n",
    "  * matplotlib\n",
    "    + visualization (2D plot)\n",
    "  * scikit-learn\n",
    "    + main algorithms for machine learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이것도 한 번 해보세요\n",
    "# import this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------\n",
    "\n",
    "## Reading Data\n",
    "\n",
    "#### 오늘 실습할 데이터를 땡겨(?)옵니다. (google cloud computing 환경을 실습하고 있기 때문에 파일을 어디서엔가 가져와야 합니다.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.github.com/jeonkiwan/PythonTutorial/master/Tutorial01/wdbc.csv -O wdbc.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "### Data Description\n",
    "\n",
    "Title: Wisconsin Diagnostic Breast Cancer (WDBC)\n",
    "\n",
    "\n",
    "* Number of instances: 569 \n",
    "\n",
    "* Number of attributes: 32 (ID, diagnosis, 30 real-valued input features)\n",
    "\n",
    "* Attribute information\n",
    "\n",
    "    + ID number\n",
    "    + __Diagnosis (M = malignant, B = benign)__\n",
    "\n",
    "\n",
    "* Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "    + radius (mean of distances from center to points on the perimeter)\n",
    "    + texture (standard deviation of gray-scale values)\n",
    "    + perimeter\n",
    "    + area\n",
    "    + smoothness (local variation in radius lengths)\n",
    "    + compactness (perimeter^2 / area - 1.0)\n",
    "    + concavity (severity of concave portions of the contour)\n",
    "    + concave points (number of concave portions of the contour)\n",
    "    + symmetry \n",
    "    + fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "\n",
    "* The __mean__, __standard error__, and __\"worst\" or largest__ (mean of the three largest values) of these features were computed for each image, resulting in 30 features.  For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.\n",
    "\n",
    "* All feature values are recoded with four significant digits.\n",
    "\n",
    "* Missing attribute values: none\n",
    "\n",
    "* __Class distribution: 357 benign, 212 malignant__\n",
    "\n",
    "--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "#### 파일을 pandas library로 불러서, 컴퓨터에서 쓸 수 있는 [데이터프레임(DataFrame)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)으로 만듭니다. (그렇다고 합니다. :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdbc = pd.read_csv('./wdbc.csv')\n",
    "\n",
    "# first 5 rows\n",
    "print(len(wdbc))\n",
    "wdbc.head()\n",
    "\n",
    "# full \n",
    "# print(wdbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdbc.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "\n",
    "## Convert DataFrame as Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column = wdbc.columns[2:]\n",
    "X = wdbc[data_column].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.Categorical(wdbc.diagnosis).codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "\n",
    "## Scikit-learn : Data Splitting\n",
    "\n",
    "#### Scikit-learn의 내장함수인 train_test_split을 이용하여 train set와 test set을 분리해 봅시다.\n",
    "\n",
    "```python\n",
    "def train_test_split(*arrays, **options):\n",
    "    \"\"\"\n",
    "    Split arrays or matrices into random train and test subsets\n",
    "    Quick utility that wraps input validation and\n",
    "    ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
    "    into a single call for splitting (and optionally subsampling) data in a\n",
    "    oneliner.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    *arrays : sequence of indexables with same length / shape[0]\n",
    "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
    "        matrices or pandas dataframes.\n",
    "    test_size : float, int or None, optional (default=0.25)\n",
    "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
    "        of the dataset to include in the test split. If int, represents the\n",
    "        absolute number of test samples. If None, the value is set to the\n",
    "        complement of the train size. By default, the value is set to 0.25.\n",
    "        The default will change in version 0.21. It will remain 0.25 only\n",
    "        if ``train_size`` is unspecified, otherwise it will complement\n",
    "        the specified ``train_size``.\n",
    "    train_size : float, int, or None, (default=None)\n",
    "        If float, should be between 0.0 and 1.0 and represent the\n",
    "        proportion of the dataset to include in the train split. If\n",
    "        int, represents the absolute number of train samples. If None,\n",
    "        the value is automatically set to the complement of the test size.\n",
    "    random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator;\n",
    "        If RandomState instance, random_state is the random number generator;\n",
    "        If None, the random number generator is the RandomState instance used\n",
    "        by `np.random`.\n",
    "    shuffle : boolean, optional (default=True)\n",
    "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
    "        then stratify must be None.\n",
    "    stratify : array-like or None (default=None)\n",
    "        If not None, data is split in a stratified fashion, using this as\n",
    "        the class labels.\n",
    "    Returns\n",
    "    -------\n",
    "    splitting : list, length=2 * len(arrays)\n",
    "        List containing train-test split of inputs.\n",
    "        .. versionadded:: 0.16\n",
    "            If the input is sparse, the output will be a\n",
    "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
    "            input type.\n",
    "    \"\"\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, Y, \n",
    "                                                    test_size=0.3,\n",
    "#                                                    train_size=0.75,\n",
    "                                                    random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train data ratio: ', len(train_X)/len(X))\n",
    "\n",
    "print('All:', np.bincount(Y) / float(len(Y)) * 100.0)\n",
    "print('Training:', np.bincount(train_y) / float(len(train_y)) * 100.0)\n",
    "print('Test:', np.bincount(test_y) / float(len(test_y)) * 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Balencing Issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X, Y, \n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=12345,\n",
    "                                                    stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('All:', np.bincount(Y) / float(len(Y)) * 100.0)\n",
    "print('Training:', np.bincount(train_y) / float(len(train_y)) * 100.0)\n",
    "print('Test:', np.bincount(test_y) / float(len(test_y)) * 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------\n",
    "## Let's Get It \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load algorithm (create classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_lr = LogisticRegression() # class instance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_lr.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_lr = classifier_lr.predict(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction_lr)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fraction Correct [Accuracy]: \", np.sum(prediction_lr == test_y) / float(len(test_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Samples incorrectly classified: ', np.where(prediction_lr != test_y)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "#### [K Neighbors Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call module (method)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# create classifier (class) instance \n",
    "classifier_kn = KNeighborsClassifier().fit(train_X, train_y)\n",
    "\n",
    "# compute accuracy \n",
    "print(\"Fraction Correct [Accuracy]: \", classifier_kn.score(test_X, test_y))\n",
    "# or \n",
    "# print(\"Fraction Correct [Accuracy]: \", np.sum(classifier_kn.predict(test_X) == test_y) / float(len(test_y)))\n",
    "\n",
    "print('Samples incorrectly classified: ', np.where(classifier_kn.predict(test_X) != test_y)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "## Show me the Graph (not money!!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define an useful function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "\n",
    "# function definition ...\n",
    "def plot_embedding(X, Y, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(X.shape[0]):\n",
    "        if X.shape[1] > 1:\n",
    "            plt.text(X[i, 0], X[i, 1], 'M' if Y[i] > 0 else 'B', color=plt.cm.tab10(Y[i]), fontdict={'weight': 'bold', 'size': 9})\n",
    "        else:\n",
    "            plt.text(X[i, 0], 0.5, 'M' if Y[i] > 0 else 'B', color=plt.cm.tab10(1-Y[i]), fontdict={'weight': 'bold', 'size': 9})\n",
    "        \n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Principal Component Analysis, PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2).fit(train_X)\n",
    "# print(pca.singular_values_)  \n",
    "# print(pca.explained_variance_ratio_)  \n",
    "# print(pca.components_)\n",
    "\n",
    "X_pca = pca.transform(train_X)\n",
    "plot_embedding(X_pca, train_y, \"Principal Components projection\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "## Pump up the accuracy !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf. full data scaling ...\n",
    "# scaled_X = (wdbc[data_column] - wdbc[data_column].mean())/wdbc[data_column].std()\n",
    "# train_X, test_X, train_y, test_y = train_test_split(scaled_X, Y, \n",
    "#                                                     train_size=0.75,\n",
    "#                                                     test_size=0.25,\n",
    "#                                                     random_state=1234,\n",
    "#                                                     stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_X = scaler.transform(train_X)\n",
    "scaled_test_X = scaler.transform(test_X)\n",
    "\n",
    "# check\n",
    "# print(np.mean(scaled_train_X,axis=0), np.std(scaled_train_X,axis=0))\n",
    "# print(np.mean(scaled_test_X,axis=0), np.std(scaled_test_X,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_skn = KNeighborsClassifier().fit(scaled_train_X, train_y)\n",
    "\n",
    "print(\"Fraction Correct [Accuracy]: \", classifier_skn.score(scaled_test_X, test_y))\n",
    "print('Samples incorrectly classified: ', np.where(classifier_skn.predict(scaled_test_X) != test_y)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2).fit(scaled_train_X)\n",
    "# print(pca.singular_values_)  \n",
    "# print(pca.explained_variance_ratio_)  \n",
    "# print(pca.components_)\n",
    "\n",
    "X_pca = pca.transform(scaled_train_X)\n",
    "plot_embedding(X_pca, train_y, \"Principal Components projection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear classifier with scaled data\n",
    "classifier_lr = LogisticRegression()\n",
    "classifier_lr.fit(scaled_train_X, train_y)\n",
    "\n",
    "print(\"Fraction Correct [Accuracy]: \", classifier_lr.score(scaled_test_X, test_y))\n",
    "print('Samples incorrectly classified: ', np.where(classifier_lr.predict(scaled_test_X) != test_y)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------\n",
    "#### [Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm\n",
    "classifier_dt = tree.DecisionTreeClassifier(criterion='gini', max_depth=None)\n",
    "# fit!\n",
    "classifier_dt.fit(scaled_train_X, train_y)\n",
    "\n",
    "score = cross_val_score(classifier_dt, scaled_train_X, train_y, cv=5)\n",
    "print(' mean:', score.mean(), ' std:', score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree graph\n",
    "import graphviz \n",
    "#\n",
    "dtc_data = tree.export_graphviz(classifier_dt, out_file=None, filled=True, rounded=True,\n",
    "                                feature_names=data_column,\n",
    "                                special_characters=False) \n",
    "graph = graphviz.Source(dtc_data) \n",
    "graph\n",
    "\n",
    "# file export ...\n",
    "# graph.render(\"wisconsin diagnostic breast cancer\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fraction Correct [Accuracy]: \", classifier_dt.score(scaled_test_X, test_y))\n",
    "print('Samples incorrectly classified: ', np.where(classifier_dt.predict(scaled_test_X) != test_y)[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------\n",
    "## [Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html) \n",
    "\n",
    "\n",
    "-------------------------------------------------------------\n",
    "#### [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm\n",
    "classifier_rf = RandomForestClassifier(n_estimators=64)\n",
    "# fit!\n",
    "classifier_rf.fit(scaled_train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(classifier_rf, scaled_train_X, train_y, cv=10)\n",
    "print(' mean:', score.mean(), ' std:', score.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = classifier_rf.feature_importances_\n",
    "# importances_std = np.std([tr.feature_importances_ for tr in classifier_rfc.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "# for i in range(len(data_column)):\n",
    "for idx in indices:\n",
    "    print(\"{:24s}  {:.4f} \".format(data_column[idx], importances[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [plt.cm.tab20(k%20) for k in range(len(data_column))]\n",
    "\n",
    "# cf.\n",
    "# colors = [] # or colors = list()\n",
    "# for k in range(len(data_column)):\n",
    "#     colors.append(plt.cm.tab20(k%20))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Feature importances\")\n",
    "ax.barh(range(len(data_column)), importances[indices], color=colors, align=\"center\")\n",
    "ax.set_yticks(range(len(data_column)))\n",
    "ax.set_yticklabels(data_column[indices])\n",
    "ax.set_ylim([-1, len(data_column)])\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
